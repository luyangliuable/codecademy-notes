Seamless Transition From Machine Learning on the
Cloud to Industrial Edge Devices With Thinger.io
Alvaro Luis Bustamante , Miguel A. Patricio , Antonio Berlanga , and José M. Molina , Member, IEEE
Abstract—Due to Industry 4.0, machines can be connected to
their manufacturing processes with the ability to react faster and
smarter to changing conditions in a factory. Previously, Internet
of Things (IoT) devices could only collect and send data to the
cloud for analysis. However, the increasing computing capacity
of today’s devices allows them to perform complex computations on-device, resulting in edge computing. Edge devices are a
fundamental component of modern, distributed real-world artificial intelligence (AI) systems in Industry 4.0 environments. As a
result, edge computing extends cloud computing capabilities by
bringing services near the edge of a network and thus supports
a new variety of AI services and machine learning (ML) applications. However, there is a large difference between designing
and training an ML model, potentially in the cloud, to create
ML services that can be deployed and consumed on the edge.
This article presents an ML workflow based on ML operations
(MLOps) over the Thinger.io IoT platform to streamline the transition from model training to model deployment on edge devices.
The proposed workflow is composed of different elements, such
as the ML training pipeline, ML deployment pipeline, and ML
workspace. Similarly, this article describes the ease of design
and deployment of the proposed solution in a real environment,
where an anomaly detection service is implemented for detecting outliers on temperature and humidity measurements. The
performance tests performed over the ML pipeline steps and
the ML service throughput on the edge indicate that this workflow adds minimum overhead to the process, providing a more
reliable, reusable, and productive environment.
Index Terms—Edge computing, Internet of Things (IoT),
machine learning operations (MLOps), Thinger.io.
I. INTRODUCTION
I
NDUSTRY 4.0 is a new industrial revolution that promotes manufacturing and industry toward processes where
new technologies, such as the Internet of Things (IoT), big
data analytics, cloud computing, or cyber–physical systems,
Manuscript received 20 March 2023; accepted 11 April 2023. Date of
publication 24 April 2023; date of current version 7 September 2023. This
work was supported in part by the Centro para el Desarrollo Tecnológico
Industrial E.P.E. (CDTI) under Grant CNU/1308/2018 (28 November); in
part by the Public Research Projects of the Spanish Ministry of Science
and Innovation under Grant PID2020-118249RB-C22 and Grant PDC2021-
121567-C22-AEI/10.13039/501100011033; in part by PEICTI 2021–2023
under Grant TED2021-131520BC22; and in part by the Madrid Government
(Comunidad de Madrid-Spain) under the Multiannual Agreement with UC3M
in the line of Excellence of University Professors (EPUC3M17) and in the
context of the V PRICIT (Regional Programme of Research and Technological
Innovation). (Corresponding author: Miguel A. Patricio.)
Alvaro Luis Bustamante is with the Engineering Team, Thinger.io,
28005 Madrid, Spain.
Miguel A. Patricio, Antonio Berlanga, and José M. Molina are with the
Applied Artificial Intelligence Group, Universidad Carlos III de Madrid,
28270 Madrid, Spain (e-mail: mpatrici@inf.uc3m.es).
Digital Object Identifier 10.1109/JIOT.2023.3268771
are integrated [1], [2]. Edge computing also plays a key role
in providing solutions to delay-sensitive processes by deploying intelligent systems closer to the data source (sensors).
Edge computing is transforming and revolutionizing how data
are being managed, processed, and delivered from billions of
devices worldwide [3], [4]. The approach moves data preprocessing, local storage, and filtering near the data sources. Highspeed networking technologies, including 5G networking [5],
enable edge computing devices to accelerate the development
and deployment of real-time applications. Such applications
include robotics, video processing, smart video analytics, artificial intelligence (AI), autonomous driving, medical imaging,
machine vision, industrial inspection, etc.
Initially, edge computing was assigned to mitigate bandwidth costs for data traveling long distances due to the exponential growth of data generated by IoT [6], particularly when
processing the data of cameras (visual sensors) [7], [8], [9].
Among a wide range of advantages, data reduction at the
network edge can prevent bottlenecks and dramatically lower
storage, energy, and bandwidth costs. Its decentralized character makes the system much more robust because the individual
edge nodes can operate autonomously and provide offline
capabilities [10], [11]. This characteristic is the reason the
use of edge devices is becoming increasingly common, particularly for integrating mission-critical AI applications, such
as those based on machine learning (ML) [12], [13], [14].
When ML solutions are built, most organizations follow
development methods, such as the cross-industry standard process model for data mining (CRISP-DM) [15], [16]. In this
method, different phases are defined: 1) data extraction; 2) data
preparation; 3) ML model building; 4) ML model evaluation;
and 5) ML model deployment. These phases are normally
executed within the cloud by leveraging large computing capabilities [17], [18]. However, running ML processes exclusively
on the cloud can be problematic in industrial IoT environments, with possible long latencies; large amounts of data
must be transmitted to the cloud; strict security policies do not
allow exposing sensitive information; or connections between
local devices and the cloud may be unreliable. This process is
how some ML processes are transitioning from cloud to edge
devices [19], [20].
However, deploying ML solutions on the edge is not trivial [21] because there are few tools available for facing
the cyber-physicality of IoT systems in industrial environments [22]. Edge devices are also limited in bandwidth,
storage, and processing power, which can limit training complex ML models. It is thus not practical to work directly on
2327-4662 c 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16549
edge devices because they are normally embedded devices
without the typical desktop computer capabilities (screen, keyboard, etc.). Thus, a hybrid cloud-edge approach is preferred
in such scenarios [23], [24], where the cloud is used for the
expensive part of ML training and validation, and the edge is
used for executing the generated model.
The goal of this article is to present a framework that
enables collaborative edge–cloud environments to automate all
the ML phases mentioned on CRISP-DM, with a focus on
enabling automatic ML model deployment on edge devices.
The proposed framework is developed within the Thinger.io
IoT platform [25], [26]. This platform is a new Open Source
platform to deploy IoT applications, which is being widely
used by the scientific and technological community [27], [28],
[29], [30], [31], [32], [33], [34].
The remainder of this article is organized as follows.
Section II discusses the related works, focusing on previous
research and literature related to the topic, and highlighting the
strengths and limitations of previous approaches. Section III
defines the problem that this study aims to solve and discusses its importance and impact. Section IV describes the
proposed solution, which is based on an ML workflow executed over Thinger.io. Section V discusses a real use case
example using the proposed workflow. In Section VI, the execution performance of the ML workflow is evaluated. Finally,
Section VII provides conclusions and outlines future work
directions.
II. RELATED RESEARCH
In the context of Industry 4.0, passing all data generated
by connected devices to the cloud server would be a larger
burden. Therefore, edge computing must be used to decongest the cloud. Conversely, for many services, it is necessary
to store all the data in the cloud for further analysis, which
requires cooperation between edge computing and cloud computing. This type of coordination has been developed in many
different sectors, including energy [35], manufacturing [36],
transport [37], health [38], and security [39].
Edge–cloud collaboration is possible in the environment
of interest and aims to reduce latency, increase scalability,
and speed-up business development [40]. However, deploying these collaborative systems is typically difficult. In [41],
coordination is approached as a dynamic resource allocation
problem, where the goal is to optimize resource allocation
by considering task priorities and network transmission costs.
Edge–cloud collaboration faces challenges such as shared
storage. Yanling et al. [42] proposed a scheme for data
replica placement to coordinate processing of data-intensive
IoT workflows in an edge and cloud computing environment.
Yang et al. [43] proposed a hierarchical architecture for
connecting and managing the elements of a manufacturing
plant to support latency-sensitive applications for real-time
responses. Collaborative edge–cloud platforms are used in
precision agriculture to manage and control cyber–physical
systems at the edge, while the cloud platform collects current and past records and hosts data analysis modules [44].
Aloi et al. [45] developed a novel platform called E-ALPHA
for ambient-assisted living services to monitor and improve
the health of an aging population, particularly in care facilities.
The platform combines edge and cloud computing, supports
different communication protocols, and is flexible and interoperable with other IoT platforms. The simulation-based design
helps assess the expected performance of the service and the
most appropriate configurations for a real deployment.
Xiao et al. [46] presented a collaborative cloud-edge service cognitive framework for deep neural network (DNN)
model service configuration to provide dynamic and flexible
computing services to adapt to different service requirements.
However, this framework does not address the problem of
building and deploying ML models to edge devices from a
technological perspective but as a theoretical decision-making
optimization problem.
All of these methods suffer from critical limitations that are
primarily related to their limited flexibility and adaptability to
different environments. Other studies report theoretical frameworks where their usefulness in real environments has not yet
been demonstrated. Current proposals lack sufficient reconfigurability, openness, and evolvability to manage deployment
perturbations in heterogeneous environments. This edge–cloud
collaboration must face scientific and technological challenges
such as the deployment of AI or ML models on the edge
side. Fortino et al. [47] presented a survey that aimed to guide
IoT developers by providing reference definitions and reviewing 70 relevant products through a comparative and practical
approach. Their survey focused on IoT methodologies, frameworks, platforms, and tools, and the review is based on general
System-of-Systems (SoSs) engineering characteristics and the
primary requirements of IoT systems, such as interoperability,
scalability, intelligence, and autonomy. They concluded that
popular IoT tools are easy to use, but their intelligence and
scalability are limited, and they rely on external cloud-based
services to perform complex processing tasks. Therefore, the
creation of a flexible infrastructure is necessary to enable a
seamless transition from building ML on the cloud to its
deployment on edge devices.
In recent years, various efforts have been made to bring
ML models to edge computing [48], [49], [50]. In the field
of mobile computing, understanding a mobile device as an
edge device (mobile edge computing), solutions have been
introduced that try to optimize ML models so that they are sufficiently light to be executed on mobile devices [51], [52], [53].
Thus, different methods have attempted to build ML models that adapt to devices with limited computational resources
[54], [55], [56]. Other studies presented solutions where (more
computationally expensive) training processes are performed
on the cloud side and are then deployed on edge devices [57].
However, the deployment of these ML models is typically performed manually, and little effort has been made to scale and
automate this process.
As discussed in Section I, life-cycle management using
CRISP-DM methodologies contains a set of phases, and each
phase has certain specific resources. For example, the learning phase requires voluminous data storage for model building
and even high computational capacities, such as CPU, GPU,
or TPU, which is not suitable for edge devices. An edge
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16550 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Fig. 1. Monolith ML pipeline, where the ML and operations are decoupled. Trained models cannot be automatically deployed, tested, and monitored.
device shown as a small computer that functions as a gateway
between a local network and the cloud, which can communicate with local devices via protocols, such as WiFi, Bluetooth,
ZigBee, or other industrial protocols (EtherCAT, Modbus,
Canbus, etc.). Edge devices communicate with the cloud using
protocols, such as MQTT, CoAP, IOTMP, or HTTP, and have
limited computational resources; thus, edge devices could not
perform all ML phases monolithically, particularly in the
expensive phases related to training. This reasoning suggests
developing a collaborative architecture between edge devices
and the cloud. In this architecture, the cloud takes care of
expensive ML phases, except the last deployment phase, which
will be performed by edge devices.
In IoT environments with the arrival of new data, a deterioration in model performance can occur, forcing models to
be retrained. Currently, many platforms support the life-cycle
management of ML applications via their different phases and
include tensorflow extended (TFX) [58], [59], ModelOps [60],
Kubeflow [61], [62], Apache Airf-low [63], and Amazon
SageMaker [64], [65]. These platforms allow the orchestration of the phases of these applications through ML pipelines.
ML pipelines aim to automate the phases of an ML application
using DevOps concepts. Thus, concepts, such as ML operations (MLOps) [66], [67] appear for the development of ML
systems that take advantage of features from DevOps [68].
As in DevOps, MLOps uses concepts, such as continuous
integration (CI) and continuous delivery (CD), to increase
developer productivity. MLOps can also incorporate concepts,
such as continuous training (CT), which occurs when a model
must be retrained because its performance has degraded or
new data become available. However, those platforms suffer from multiple constraints: they are limited to a single
ML framework; they require a cloud to be executed and
so cannot be executed on-premise servers; and they require
complex environments for its execution (i.e., Kubernetes).
Thus, setup costs are high, or solutions are not sufficiently
open and lack good community support rather than technical support, which limits their applicability in scientific and
academic environments. Those approaches are not focused on
cloud-edge collaboration and thus do not provide a complete
ML workflow because edge deployment should be performed
manually.
Therefore, the motivation of this study is to propose a collaborative architecture between edge devices and the cloud
based on IoT environments. In this architecture, the cloud takes
control of all ML phases, except the last phase of deployment,
which will be performed by edge devices. This architecture
solves common limitations when running ML in the cloud in
terms of latency or reliability of connections. In addition, this
architecture automates the deployment of ML models to edge
devices using the latest software standards principles inherited
from MLOps, which improves the security, sustainability, and
heterogeneity of ML deployments. Using edge devices within
an IoT framework can also provide a solution that includes CT,
evaluation, and performance tuning because devices can provide new measurements that can be used to create improved
models over time. Therefore, the latest motivation is to obtain
a comprehensive solution for building ML models on the
cloud that can be easily deployed and executed in edge
devices.
III. PROBLEM STATEMENT
The problem that this study considers shortens the gap
between ML engineering and ML deployment in the field (i.e.,
running and updating ML models on edge devices operating
in industry environments).
Building an ML system consists of multiple sequential
steps, which are often referred to as a pipeline, that do everything required to generate a model, which usually involves
data ingestion, cleaning, preprocessing, and training. In a
mainstream system design, all of these tasks would be run
together in a monolith; thus, the same script will extract the
data, perform cleanup and data preprocessing, train a model,
and perform some evaluations. Because ML models typically
consist of far less code than other software applications, the
approach to keep all of the assets in one place makes sense.
Fig. 1 represents this schema, where a single ML script or
ML notebook performs all the required processing to build a
model.
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16551
Fig. 2. Automated ML pipeline on Thinger.io. ML and operations are integrated. Trained models can be automatically deployed. The ML pipeline has access
to IoT sensor data or any metadata required for running the ML scripts.
The generated ML model is often transferred manually to IT
Ops teams, as ML and Ops are completely decoupled. Thus,
the Ops team is responsible for creating a prediction service
(i.e., over a REST API). This decoupled workflow can be useful in the first integration steps. However, once teams move
away from a stage where they are occasionally updating a
single model to having multiple frequent model updates in
production, a manual or monolith pipeline approach becomes
paramount. Some problems that arise are as follows.
1) Creating variations of the same model requires running the complete pipeline manually again, even though
the first steps of ingestion and preparation are exactly
identical.
2) Creating new models typically involves copying and
pasting code from the beginning stages of the pipeline,
which is inefficient and a poor sign in software
development.
3) Changing the configuration of a data source or other
commonly used parts of the pipeline involves manually
updating all related scripts, which is time-consuming and
can produce errors.
4) Data scientists produce a trained model as an artifact to
the Ops team, which must deploy it to the API framework. This process involves manual steps that cannot
be scaled easily, particularly when this model should be
deployed to several edge devices.
5) There is a lack of active performance monitoring once
a model is deployed, which is necessary to detect
performance degradation and other deviations in model
behavior.
In an ideal scenario, the ML steps (data extraction, data
preparation, model training, model evaluation, etc.) can be sufficiently automated to have a reproducible environment that
generates and validates ML models within a few clicks. To
address these challenges, the field of MLOps has emerged
to improve the deployment of ML systems in a reliable and
efficient manner. However, integrating MLOps practices with
edge devices in IoT environments remains a challenge. Thus,
this article presents an ML workflow that automates and monitors all stages of the development and deployment of ML
systems in IoT environments, with a specific focus on edge
computing within the Thinger.io platform.
IV. MACHINE LEARNING WORKFLOW
This section describes the proposed ML workflow using
the Thinger.io IoT platform. The primary goal of this workflow is to automate the ML pipeline, both for training and
updating ML models and for deploying it automatically to
edge devices. Thus, this section is divided into three parts:
the first is related to the pipeline designed for training ML
models and obtaining a trained model artifact; the second
discusses how to pack, publish, and deploy trained models
to both edge devices or other cloud services; and in the
third, the proposed ML workspace is presented to unify the
design and development of ML pipelines with a graphical user
interface (GUI).
A. ML Training Pipeline
In the proposed automated workflow, solid engineering principles become more important. The ML scripts are split into
more manageable and independent components, such as data
extraction, data preparation, model training, or model evaluation. There is no single offline data source or fixed parameters
in the script. Instead, the ML pipeline can access IoT sensor data or any metadata required for running the ML scripts,
as the workflow will be running on the cloud with access to
several IoT resources. The ability to split the problem solving into reproducible, predefined, and executable components
forces the team to adhere to a joined process. Conversely, a
joined process creates a well-defined language between the
data scientists and the engineers and eventually leads to an
automated setup that is the ML equivalent of CI, a product
that can autoupdate itself.
The proposed ML pipeline is outlined in Fig. 2. In this
approach, the ML pipeline does not rely on a single ML script
or notebook. Instead, each step of the pipeline is converted
into an individual module. Each module in this scheme should
be reusable, composable, and potentially shareable across ML
pipelines. Thus, while the ML code can still live in notebooks,
the source code for pipeline components must be modularized.
The components described in this picture are the following.
1) Data Extraction: This component will be responsible
for taking data from IoT sensors (i.e., live databases
that can be queried over the REST API). The result of
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16552 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
this component will be a set of files with the raw data
extracted from the database.
2) Data Preparation: In this step, data are prepared for the
ML task, and this preparation involves data cleaning and
splitting the data into training, validation, and test sets.
Also, this preparation can involve data transformations
and feature engineering to the model that solves the target task. The output of this step is the data splits in the
prepared format.
3) Model Training: The data scientist implements different
algorithms with the prepared data from the previous step
to train an ML model. The output of this step is a trained
model.
4) Model Evaluation: The model is evaluated on a holdout test set to evaluate the model quality. The output
of this step is a set of metrics to assess the model
performance. The model can be validated in this step,
where its predictive performance should be verified to
be better than a set baseline.
5) Model Serving: The validated model is deployed in a target environment to make predictions. This deployment
can be done in different ways, including as a microservice with REST API to serve online predictions or an
embedded model to and edge device.
6) Model Monitoring: The model predictive performance
is monitored to potentially invoke a new iteration in the
ML process.
7) Pipeline Triggers: The ML pipeline can be executed
again depending on multiple factors (e.g., ad hoc manual
executions of the pipeline, on a schedule, on availability of new training data, and on model performance
degradation).
With this approach, it is possible to perform CT of the
model by automating the ML pipeline, allowing CD of
model prediction service, even for edge devices. This process is often referred to as MLOps. MLOps is an extension of the DevOps methodology that seeks to include ML
and data science processes in the development and operations chain to make ML development more reliable and
productive. The objective of MLOps is to develop, train,
and deploy ML models with automated procedures that
integrate the data, developers, security, and infrastructure
teams.
To automate this type of ML pipeline, we standardize the
process by creating a well-defined language and framework
between the data scientists and the engineers. This process
begins by defining the exogenous elements from the ML
pipeline that are required for its execution (i.e., training data,
configurable parameters, or scripts). In Thinger.io, these elements can be covered with native features that are tied to the
ML pipeline. These features are described below.
1) Time-Series Buckets: All data from IoT devices are
stored in time-series databases, which allows the ingest
scripts to query data by date, apply aggregations (mean,
average, max, min, etc.), filter data by sensor ID, filter
by asset location, etc.
2) File Storages: All ML scripts and intermediary artifacts can be stored on File Storages. Each ML pipeline
step has access to a native file storage managed by the
platform that can be used to read or write any files.
3) Metadata: ML scripts can obtain running parameters
and configurations from generic metadata stores that are
applied to each pipeline instead of being hard-coded in
the script. These metadata can be accessed over REST
API, environment variables, script parameters, etc.
4) Events: Thinger.io includes an event manager that can
trigger the ML pipeline when required, i.e., changes on
scripts, performance degradation, manual trigger, etc.
5) Monitoring: Thinger.io is primarily used to ingest data
from any source, create monitoring dashboards, and
create alerts or events based on incoming data. The
ML model performance is just another data source
to be monitored with Thinger.io, which can generate
triggers based on custom conditions. Thus, automatic
ML retraining could be configured based on monitoring
parameters.
Once all required elements to run the ML pipeline are
available, each component must be executed sequentially.
This process may appear trivial because it only requires running the ML code. However, this process is actually more
difficult because each ML pipeline can be created in different languages, with different frameworks, or specific library
versions, depending on the ML scientists” preferences or
project requirements. Then, a generic ML pipeline can become
difficult to maintain and develop in a real Industry 4.0
environment, particularly when there are several target edge
devices with different hardware architectures.
To solve this problem, we use the concept of containers for each ML pipeline step. A container is a standard
unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one
computing environment to another. A container image is a
lightweight, standalone, executable package of software that
includes everything required to run an application: code, runtime, system tools, system libraries, and settings. Containers
are an abstraction at the app layer that packages code and
dependencies together. Multiple containers can run on the
same machine and share the OS kernel with other containers,
each running as isolated processes in user space. Containers
take up less space than VMs (container images are typically
tens of MBs in size), can manage more applications, and
require fewer VMs and operating systems.
Using containers, the ML team can define an ML framework with its dependencies and specific versions. For example,
it is possible to create a container image with R, TensorFlow,
TensorFlow+Keras, SciKit, or any other requirement to execute the ML scripts. Many ML frameworks are already built
by the community; thus, in many cases, it is not necessary to
create new ones. In the worst case, they can be extended with
additional libraries or components.
Creating or extending container images is something that
can be done even by ML scientists. For example, the code
snippet shown in Listing 1 extends FROM an r-base image,
which packages the R framework as ready to use. Then, it RUN
an R script to install a library to read “xlsx” files. This file
can be easily “compiled” into an image (e.g., with Docker),
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16553
Fig. 3. ML training pipeline on Thinger.io based on custom Docker ML training images. ML scripts are executed inside Thinger.io and may use resources,
such as time-series data, file stages, or other metadata and configuration.
Listing 1. Example image definition (Dockerfile) for R framework with
custom libraries.
tagged with a name, and eventually pushed to a central image
repository; thus, other ML scientists can reuse exactly the same
framework. Once an image is built, it is possible to run any R
script over them. In this case, the scripts of the ML pipeline
are executed over such images. Similarly, this file is defined as
a base container image for R, and it is possible to create any
other image for TensorFlow, PyTorch, Keras, etc. It is even
possible to extend this file from a base operating system and
include any other custom code or libraries.
The example image definition described above, which is
typically a dockerfile, can be considered an ML training image,
packing all dependencies required for executing the ML steps
in the pipeline. Then, running the ML training pipeline is just
running the pipeline scripts in sequence over the ML training
image(s). As described in Fig. 3, required Thinger.io resources
are exposed to the ML pipeline (i.e., via API REST, environment parameters, or direct mount volume when running the
ML training image with each pipeline script).
Once this pipeline is executed, the result obtained is a
trained model and training results (i.e., validation results
or model performance). Those results are also stored in
Thinger.io resources, such as file storage or metadata stores.
This pipeline can also be executed both in Thinger.io cloud or
on-premise, which are common in Industry 4.0 environments.
Thinger.io does not impose using a particular cloud because
it only has dependencies with other open-source projects such
as Linux, MongoDB, InfluxDB, and Docker.
B. ML Deployment Pipeline
The final result from ML training is just a model and a set
of evaluation metrics. Deploying a model usually consists of
serializing a variable (e.g., a Python object or a TensorFlow
model) into a format that allows the variable to be stored or
transmitted. Deploying such serialized models into production is not an automatic task and typically involves sending
the model artifact to the Ops team, which in turn builds
a prediction service over it. Building a prediction service
involves several steps, such as configuring an ML framework,
loading the model via deserialization, developing prediction
functions, and building an API around them.
This type of deployment can be assumed when there are
infrequent release iterations. The process assumes that the
data science team manages a few models that do not change
frequently, either by changing model implementation or by
retraining the model with new data. However, this process is
unreasonable when the model must be deployed to several targets (e.g., edge devices) or when the model evolves over time
and must be retrained, which can be the case when using a
live data source such as in IoT devices.
Thus, an automated ML deployment pipeline in consonance
with the automated ML training pipeline described in the
previous section is desirable. Thus, every time a new model
is generated and validated, the pipeline can be automatically
deployed. Therefore, the system continuously delivers new
pipeline implementations to the target environment that in turn
delivers prediction services of the newly trained model.
The deployment pipeline follows a similar pattern to the
training pipeline. Similarly, the training pipeline is based on
the ML training image, and in the deployment pipeline, an
ML service image is also defined. This ML Service Image is
a composed image from a Base Service Image containing an
ML framework, such as TensorFlow, PyTorch, Keras, etc., plus
additional files such as the model, default execution parameters, and required files for running the prediction service. The
composed image is a fully packaged application that is stored
on an image registry for delivery to target environments, such
as edge devices, clouds, kubernetes, etc., as shown in Fig. 4.
The final result of this deployment pipeline is not a model
but rather a complete image that can be stored, versioned, and
deployed to several target environments. Then, an ML learning implementation on Thinger.io consists of both the scripts
required for training models and the scripts to serve the model.
In this way, ML engineers can work together with IT engineers under a unified ML workflow to provide complete ML
solutions from start to end.
Service scripts are part of the ML application and are
responsible for loading the model, loading execution parameters, and providing a service to be consumed by other
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16554 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Fig. 4. ML deployment pipeline on Thinger.io where the final artifact is an ML service image for different target architectures. The image is stored on an
image registry, where it can be downloaded by edge devices or other deployments.
applications or services. The expected steps for the service
script(s) are the following.
1) Load Model: The resulting model from the previous
learning pipeline is loaded into the ML Service Image.
The script is responsible for loading the model (i.e., via
deserialization or custom framework functions).
2) Load Parameters: If the model requires parameters
for its execution, they can be loaded from Thinger.io
resources (i.e., from metadata sources via REST API)
or loaded directly into the ML Service Image with files,
environment variables, etc. The script should process and
apply the parameters to the ML scripts.
3) Predict Functions: Once the model is created, predict
functions must be defined (i.e., by specifying input
parameters and output results). The prediction function must use the loaded model with the configuration
parameters.
4) Serve Model: The final step relies on exposing services
to consume the predicted functions, i.e., creating a REST
API wrapper for each predicted function. The preferred
method is REST API because it is a standard that can
be consumed from multiple solutions and can be easily
used on clouds in addition to edge devices.
Once the ML Image Service is built, it can be easily instantiated and queried using the exposed service. The
unique requisite for running ML image services is an environment supporting containers (e.g., using Docker). Docker is
currently available for Windows, Linux, and Mac and supports multiple target architectures such as x64, arm64, or
x86. Industrial edge devices are used to work on arm64,
with Linux as the operating system. Thus, the only requirement to run the image is to compile it for the target edge
device architecture. Docker released BuildX, which is a plugin that includes the ability to simultaneously build images for
multiple architectures via either a QEMU emulation or a native
device. Device architectures that can be built using the QEMU
method include Linux/arm64, Linux/riscv64, Linux/ppc64le,
Linux/s390x, Linux/386, Linux/arm/v7, and Linux/arm/v6;
thus, compatibility is nearly assured. A series of “Docker-first”
Linux distributions targeting embedded IoT platforms have
emerged. They offer a specialized environment for running
Docker-compatible containers on IoT devices and integrate
various features and tools to assist in their development and
deployment (such as BalenaOS or HypiotOS), but it is not a
requirement for deployment.
Therefore, deploying the ML solution on the edge device
relies on just running the generated ML Service Image.
Running an image is quite similar to running an application.
The underlying container engine (e.g., Docker) is responsible
for pulling the image from the associated image registry and
executing it according to its definition. In this case, the image
instance, which is referred to as the container, would expose
a service that can be used by other processes or services.
With Thinger.io, there is typically an agent running on the
edge device. This agent is responsible for interacting with raw
device resources such as sensors or actuators. In a basic setup,
this agent is responsible for collecting device information and
sending it to a remote Thinger.io platform. Thus, the platform
is used to store the received data in a time series database,
generate alerts, plot data in live dashboards, etc. The platform
can also be used to interact with the device in real time (e.g.,
actuate over the device to change its state, configuration, or
running parameters), as shown in Fig. 5.
In this setup, the Thinger.io agent is also used to initialize
and update ML environments dynamically. For example, once
a model is generated on the platform and stored in the image
registry, it is possible to command a set of devices to instantiate an ML service image and bind some device resources
(i.e., sensor readings) to the exposed ML service. The result of
executing the ML service can be used to generate new events,
alarms on the platform, or trigger actions on the device (e.g.,
when an outlier is detected).
The benefits of this approach are that the device can
execute the ML algorithms locally, without having to
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16555
Fig. 5. ML edge deployment with Thinger.io. Edge devices can download and update ML service images over the ML image registry. The ML image
download and update can be triggered directly by the Thinger.io platform using the agent deployed on edge devices.
send all measurements to the cloud, saving bandwidth and
cloud resources, and obtaining a better latency on the
results.
C. ML Workspaces
Both ML training and deployment pipelines described
in previous sections can be implemented by ML scientists
inside the Thinger.io platform using ML workspaces. An
ML workspace is a centralized place for building end-to-end
ML solutions, supporting any of the required stages, such as
data analysis, model building, deployment, or performance
monitoring. An ML workspace provides a set of tools that
streamline the design and development of ML applications,
including the following features.
1) Online Editors: Each ML workspace provides a File
Storage where it is possible to upload and edit ML
scripts, download sensor data, store intermediary artifacts, training results, dockerfiles, etc. File storage can be
managed with fully fledged online editors, such as Visual
Studio Code, including support for using interactive
Linux terminals.
2) Notebooks: An ML workspace can be opened with a
Notebook to simplify data analysis, script refinement,
model testing and evaluation, etc. Thinger.io supports
installing additional plugins that can be used within an
ML workspace, such as Jupyter Notebook.
3) Low-Code Pipelines: To simplify the development of
ML pipelines, ML processes can be designed and executed with a low-code orientation. With this approach, it
is possible to drag and drop predefined nodes to a canvas and link them together to create different pipeline
sequences (e.g., for building, deploying, and monitoring
the ML application). There are different prebuilt nodes
for data extraction, pipeline step execution, build container images, deploy images to image registry, or notify
devices about updates. It is also possible to use other
nodes to trigger pipeline execution, such as based on
platform events or based on a schedule. It is possible
to execute any step of the pipeline manually, and the
pipeline execution can be debugged in real time over a
terminal.
4) REST API: An ML workspace can be controlled via
REST APIs (e.g., for triggering pipeline executions,
running trained prediction functions, querying training results, or getting performance information). Thus,
ML applications can be integrated with other external
applications and processes in a standard and secure way.
Fig. 6 provides a screenshot of an ML workspace. ML
workspaces are integrated along with other IoT platform
resources, such as device management, data storage, and dashboards. This process provides a centralized IoT platform where
it is possible to configure, develop, and monitor IoT applications, including ML integrations. This picture shows an
example of the low-code pipeline editor, where different nodes
are linked together to create a training ML pipeline. The
ML workspace provides different sections for accessing the
Notebook, File Storage, or switching between the different
ML pipelines.
V. USE CASE EXAMPLE
In this section, a use case for building an ML pipeline for
outlier detection is presented. To show how the anomaly detection process works in the proposed framework, real sensor
readings in two scenarios (indoor and outdoor) have been used.
The data are collected by humidity–temperature sensors using
Crossbow TelosB motes [69]. The ML model will be based on
the principal component analysis (PCA) [70] algorithm. PCA
is a widely used statistical technique for multivariate analysis.
Its conceptual simplicity coupled with its robustness makes it
applicable in many situations in ML. PCA can be applied as an
unsupervised technique to perform dimensionality reduction,
attribute selection, and pattern search. PCA’s behavior is analogous to a linear autoencoder and can therefore be applied
to solve the same type of problems as these ones. PCA is
based on finding a rotation of the multidimensional space of
input variables to maximize the variance of the data projection
on these new axes. Because PCA requires few computational
resources for a reasonable number of variables to obtain the
model and to apply it, it is well suited to be applied in devices
with limited capabilities, such as edge devices. This situation
is common in IoT-based applications. There are a large number of proposals that apply PCA to this domain, particularly
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16556 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Fig. 6. Thinger.io ML workspace. ML processes can be designed and executed with a low-code orientation. It is possible to drag and drop predefined nodes
to a canvas and link them together to create different pipeline sequences.
for anomaly detection [71], [72], [73], [74], [75], [76]. The
anomaly detection system as a whole does not need to be
limited to a single anomaly detection algorithm; more techniques, such as those based on neural networks, statistics or
time series, may be incorporated later in the proposed framework. For most applications, it may be sufficient to use the
rule of considering as an anomaly (i.e., the outlier detection
problem) those data that are away from six standard deviations
above the mean; this step will be the default behavior. The
system can be used by people without any technical knowledge, even for people with them, when the system begins, it
is possible that they do not know what will be the behavior
of the sensor and what values can be considered anomalous.
The data are collected from four motes: two indoors and
two outdoors. Each of them measures the relative humidity
and temperature. Different configurations are possible to detect
possible data anomalies occurring in the motes. A model for
the entire network would be trained by grouping all the measurements in columns (e.g., a matrix of 2 measurements ×
4 motes = 8 independent variables). The number of rows
corresponds to the number of measurements. In this case, a
measurement is taken every 5 s for 6 h, i.e., a total of 4320
readings.
Another possibility is to group indoor–outdoor or
temperature–humidity motes. In this case, it would be necessary to train two models with a data set of 4 columns ×
4320 rows. There is also the possibility of having an independent model per mote and even one per measurement; in this
case, eight models should be trained. For the univariate case,
PCA cannot be applied, and instead, statistical models such as
interquartile ranges, median absolute deviation, or ML models
such as K-nearest neighbors or isolation forests can be used.
For this example, we use a model for each mote, and in
addition to the instantaneous temperature and humidity values, their gradients have also been considered. Therefore, four
models must be calculated, and each of them will need a
matrix of 4 columns × 4320 rows to calculate the principal
components. For the deployment stage, we assume that the
trained models will be integrated and deployed on an edge
device acting as a gateway for the motes, ending data to the
gateway via Bluetooth.
A. ML Training Pipeline
In this section, how the ML training pipeline is configured
inside an ML workspace for the mentioned anomaly detection use case is described. For simplicity, this section will
not discuss some parts of the training pipeline, including data
extraction, data preprocessing, and model evaluation.
ML pipelines are not fixed and can be composed of any
number of steps required by the ML engineer. All of the ML
pipeline steps defined inside the ML workspace are executed
sequentially. If any of the pipeline steps fail, the pipeline execution is stopped. In the example shown in Fig. 7, a pipeline
consisting of three different steps is created: 1) data preprocessing; 2) training; and 3) evaluation. The pipeline misses a
data extraction node at the beginning of the pipeline because
the data have been uploaded directly to the ML Workspace File
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16557
Fig. 7. ML training pipeline configured on ML workspace. This pipeline consists of three nodes for preprocessing, training, and evaluation. Each node is
configured to execute an R script over an R Docker image.
Storage and an existing data sheet is being used for illustration
purposes.
Each step on the pipeline defines its name, the command to
execute, and the base image where the command is executed.
In this case, the base image is directly a community container
image that provides the R framework. Computing the PCA
can be done with a basic R framework; thus, a custom image
with additional libraries is not required. Thus, it is possible
to place r-base:4.2.1 directly on the base image name of each
pipeline step. The image name is composed of both the image
name and the release version (4.2.1, which is an image tag).
Setting specific framework versions is a recommended practice to ensure experiment repeatability if anything changes on
the newest image versions. The r-base image is available on
the DockerHub registry,1 and the ML Workspace will pull it
automatically before its execution. It is possible to use different base images (and frameworks) for different steps of the
pipeline if required (e.g., use Scikit for data preprocessing and
R to train the model).
The pipeline can also use user-defined parameters on its
execution. These parameters can be set manually or via API
request on each execution. Thus, each pipeline execution
defines a context where user parameters can be initialized and
used in the different steps without hard-coding or modifying it
on scripts. Regarding the example, it uses a parameter called
“mote” by placing the placeholder {mote}, which is replaced
automatically on runtime. Thus, the same pipeline can be executed with different parameters to train a model for each mote
because for this example, we use a model for each one. For
example, using “mote1” as a value for the mote parameter, the
final command for the pipeline step executes train.R file,
will be “RScript –verbatim train.R mote1.” Context parameters can be used for any purpose, such as training parameters,
thresholds, or data source filtering.
Following the example, the code Listing 2 shows the training step of the pipeline (train.R), where it performs a PCA
on the given data matrix and returns the results as an object.
This object is then serialized and stored in a file for building
the deployment image in the following step. This script uses
the mote parameter on the execution to determine the source
training file to use when building the model and the target file
where the resulting model will be stored.
1https://hub.docker.com/_/r-base
Listing 2. Example script for building and storing a PCA based on previous
processing steps.
The code required to create one PCA model (2 lines of
code in R) takes 0.012 s to execute, and the model obtained
occupies 3 kB of memory. In this case, the model is a 4 × 4
matrix of real values that encodes the rotation to be applied
to each coordinate corresponding to the variables used. These
rotated axes are the principal components that are calculated
from the eigenvectors of the covariance matrix of the variables.
The model also includes the means and standard deviations of
each variable because, prior to the calculation of the covariance
matrix, Z score normalization is applied to the data. Means
and variances are recorded so that the normalization can be
reversed to present the results on the original data scale.
We cannot show all results of the four models obtained
for anomaly detection because the objective of this study is
not focused on demonstrating the performance of PCA for
anomaly detection; this fact has already been established and
demonstrated in the literature. Rather, we describe the ease
of integration of this technique together with its suitability in
terms of low use of computational resources. The reported
models correspond to mote 1 (indoor) and mote 4 (outdoor).
When using PCA to reduce dimensionality, it is necessary
to set the number of principal components to be selected.
In this case, it is not necessary, and all components are
retained. The percentage of variance captured in each principal component associated with methods 1 and 4 are shown
in Fig. 8(a) and (b).
There are marginal differences in the amount of data variance achieved by the principal components of each method.
Fig. 9(a) and (b) shows the measurements taken on each
mote in its first two principal components. The criterion used
to determine if a measurement is anomalous is when its value
exceeds the distance in any of the projections in the principal
components, calculated as six times with the median divided
by the mean absolute deviation. Listing 5 shows the code for
the distance calculation and the anomaly detection condition.
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16558 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Fig. 8. Percentage of variance explained by principal component: (a) mote 1 and (b) mote 4.
Fig. 9. Projection of the data in the first two principal components: (a) mote 1 and (b) mote 4.
Using the median and mean absolute deviation, rather than
using the mean and standard deviation, provides greater robustness to the method. Using the mean and standard deviation is
valid if the variables are normally distributed, and this hypothesis cannot be assured a priori for any sensor deployment.
There are other possible criteria, for example, the Mahalanobis
distance and Tukey’s rule. that can be easily incorporated and
used as an ensemble of criteria to reduce bias and decision
variance.
The green dots correspond to normal measurements, and
the red dots correspond to measurements considered anomalies. The image on the left is obtained with a detection level
higher than 6 and on the right with exactly 6. This image
shows how the variation in detection is different depending
on the mote. The same change impacts mote 1 more than
mote 4, which implies that it will always be necessary for the
system to inform the user about the model result so that they
understand what detection level they are using. This result
also indicates that evaluations of the results will must be
scheduled periodically to decide if retraining the models is
necessary.
Principal component space representations are useful to
describe knowledge and experience in using this technique
but can be confusing if these are not available. To facilitate
understanding of the model results for inexperienced users,
we reverse the scaling of the variables and show the values
recorded by the sensor. Highlighting with a different color is
also used when an anomaly occurs [see Fig. 10(a) and (b)].
In the top rows of Fig. 10(a) and (b), the data labeled as
anomalies are shown in red, and in the bottom rows are those
produced by the model. All labeled anomalies and abrupt discontinuities in the measurements are detected due to the effect
of adding the gradients of the variables.
B. ML Deployment Pipeline
Deploying the ML model trained in the previous step via
containers follows a similar pattern to the training step and can
be performed by building an ML deployment pipeline inside
an ML workspace. The deployment pipeline can be executed
automatically after model training and validation or any other
required event. In the pipeline shown in Fig. 11, there is an
example of a pipeline that is listening for event triggers; in
this case, an ML workspace validation succeeds. Once the
event is triggered, the model executes the following pipeline
steps: image building, image deployment, and target device(s)
notification with the new ML Service image release. All steps
are described in more detail in the following.
The first step on the deployment pipeline except the trigger
node is a node for building an ML Service Image. As described
in the ML Workflow proposal, the ML Service Image is an
image containing the base framework plus the required scripts
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16559
Fig. 10. Comparison of anomaly detection versus ground truth: (a) mote 1 and (b) mote 4.
Fig. 11. ML deployment pipeline configured on ML workspace. It is automatically triggered when the ML evaluation succeeds. Then, the pipeline builds
and deploys the ML image to a Docker registry. If everything is satisfactory, the pipeline notifies target devices to update the deployment.
and trained model from the previous stage. The node requires
some parameters, such as the target image name, target image
tag, dockerfile that specifies the image build, and target architecture. The target image name is an arbitrary name that can
give regarding the project, service, etc. In the given example,
the image name is thinger/ml_motes_anomaly. The
image tag in this case uses the value “1.0,” representing the
ML release version. The Dockerfile input is just the dockerfile file name on the ML Workspace storage that will be
used when building the image, i.e., service.dockerfile.
Finally, there is a configuration related to the target image
architectures (e.g., linux/amd64 for running on the cloud
and linux/arm64 for running on edge devices).
To show how to define a dockerfile for building the ML
Service Image, an example is presented in Listing 3. In this
case, the method uses the same base image r-base:4.2.1
used in the training stage and then installs a library called
Plumber. Plumber allows APIs to be created by merely decorating existing R source code with comments. Then, the trained
models and some helper scripts are copied to the ML Service
Image. Finally, it is defined as the primary entry point for the
image, which is the script run.R.
The entry point for the ML Image Service just loads
the Plumber library, the predict scripts on predict.R and
begins an API based on the definitions api.R, as shown
in Listing 4.
Listing 3. ML service image definition (service.dockerfile).
The predict.R initializes the predicted functions that
will be exposed by the service, as shown in Listing 5. The
is_outlier function performs the calculation on a measurement to determine if it is an outlier. The function takes
as parameters the measurement vector, the decision level, the
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16560 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Listing 4. Entrypoint for the ML image Service (run.R).
Listing 5. ML image service predict functions.
Listing 6. ML image service API definitions.
matrix, and the PCA model, which rotates the variable space to
the principal component space. The function returns a Boolean
value indicating whether or not it is an anomaly, the projection
of the measurement in the principal component space, and the
prediction; if all the principal components are used, it would
coincide with the vector of input measurements. This function
occupies 79 kB of memory and takes 0.0026 s to execute for
an input vector of dimension 4. This issue is a limitation to
the maximum data acquisition frequency of a sensor, which is
approximately 380 Hz.
Finally, the api.R script shows that the previously defined
prediction function (is_outlier) is accessible over a POST
API request on is_outlier, as described in Listing 6. Each
API execution requires various parameters, such as the mote
identifier (e.g., mote1, sensed temperature and humidity) and
a distance threshold. The API is then responsible for loading
the corresponding trained model and executing the prediction
function over it.
Once the container is built and running, it can be queried
directly over API REST. The example shown in Listing 7
shows querying the ML services using curl, passing a mote
identifier, temperature, humidity, and threshold as parameters. The service then returns the prediction result in the
isOutlier variable.
After this step, the image is ready for deployment on either
cloud or edge devices. Following the pipeline configuration in
Listing 7. ML Image Service API execution.
Fig. 12. Execution times of the ML pipeline steps.
Fig. 11, the next step is a deploy image node, which pushes the
generated image/tag to a Docker Registry of choice. For example, it is possible to push images to a private image registry
running inside the organization or on-premise or just publish it
directly to the cloud on public repositories such as DockerHub.
The end node is just a node for notifying devices about model
updates. The node in this example has been configured to
notify only the target edge device using the device identifier “motes_gateway” in the configuration. Depending on
the configuration (e.g., building the same ML model for a set
of devices), it can be used to notify device groups within a
product, an asset type, or an asset group.
VI. ML WORKFLOW PERFORMANCE
The execution time of the ML pipeline described in the
previous use case was measured for all the intermediary steps.
The pipeline execution is performed on a discrete cloud server
with 4 CPU cores and 16 GB of RAM, which includes the
Thinger.io server used for device notifications. For the edge
device, an industrial IoT device based on a Raspberry Pi Core
called Kunbus Revolution Pi2 is used. The time duration was
measured using the time command from Linux terminals.
Fig. 12 shows the execution times of each step, which are
described below.
1) Train: The duration of the train step highly depends on
the specific problem, algorithms used, and data set size.
In this study, the PCA example described in the previous
sections is not complex, and the train step required only
8.188 s to complete. This time includes running the
training code inside a Docker container.
2) Build: The build step generates the ML service
(a Docker image) using the training result from the
2https://revolutionpi.com/
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
BUSTAMANTE et al.: SEAMLESS TRANSITION FROM MACHINE LEARNING ON THE CLOUD TO INDUSTRIAL EDGE DEVICES 16561
Listing 8. ML rest API benchmark command using siege.
previous step. This step in this use case required 1.4 s
to complete. This result is obtained after a preliminary
pipeline execution where it is initialized and cached
the most expensive part of the step, which installs and
updates additional packages to the base r-base:4.2.1
image, which is time consuming. Using a cold build
without caches required 457.4 s to complete.
3) Push: The push step duration primarily depends on the
Internet bandwidth because it requires transferring generated ML images to an external Docker registry. The
push step took 21.264 s to complete for the 399.74 MB
of image size.
4) Notify: The notify step is a quick step that simply sends
a notification to edge devices, telling them that the
pipeline has been executed and that there is an updated
ML image ready for production. The notify step took
only 0.13 s to complete.
5) Pull: The pull step duration primarily depends on the
Internet bandwidth because it requires the transfer of
ML artifacts from the Docker registry. This step is run
on the edge device, where the pull step took 17.062 s
to complete.
6) Run: The run step is the final step of the pipeline and
involves running the ML model inside a Docker container in the edge device. The run step took 1.267 s to
complete.
These results demonstrate that the overhead of using an ML
pipeline based on Docker containers for training, building, and
deploying ML artifacts is not expensive (e.g., all steps are executed in under a minute), considering how much time would
be required to execute all the mentioned steps manually. The
pipeline also reduces the possibility of human errors, as all
of the steps from training to deployment are parametrized and
automated within the workflow.
Once the Run step is executed on the edge device, the
service is ready for use over a REST API. In this case,
the overall ML service performance is measured using the
siege3 benchmark tool for a minute and will execute multiple
HTTP requests simultaneously to the ML service, which
will execute the prediction function using some reference
values. The command used for this test is described in
Listing 8.
The results from this analysis reveal a high throughput
for the prediction function. In particular, this function can
resolve 207.45 requests per second, with more than 12k
requests resolved in a minute, which is more than sufficient
for the proposed use case. The complete benchmark results
are outlined in Table I.
3https://linux.die.net/man/1/siege
TABLE I
PERFORMANCE METRICS
VII. CONCLUSION AND FUTURE WORK
In this article, we proposed an ML Workflow for Industry
4.0 where MLOps development practices have been introduced
for the design and deployment of ML solutions in a collaborative edge–cloud environment based on IoT technologies. A
theoretical workflow of this new framework has been presented
and consists of two primary pipelines: 1) the ML training
pipeline and 2) the ML deployment pipeline. Then, an ML
workspace is conceptualized to easily design and configure
the pipelines using a low-code approach. Although there are
currently solutions that allow the orchestration of the phases
of an ML application (TFX, ModelOps, Kubeflow, etc.), all
of them required complex execution environments, entailing
a high configuration cost, and cannot provide an end-to-end
solution including automatic edge deployment. To describe the
operation of the proposed ML Workflow, an anomaly detection
service was designed and deployed in a real environment. The
user-friendliness of the service was tested by including a lowcode approach for designing the pipelines. A solution based
on a set of scripts in R language that implements a method
for anomaly detection using PCA has been integrated into the
problem’s solution. In addition to achieving good results,
the ease of design and deployment of these scripts within
the proposed workflow was demonstrated. The performance
of the proposed solution was tested in terms of overhead and
demonstrated that the proposed ML Workflow has a low complexity that adds only a few seconds to build and distribute
the resulting ML images from/to the cloud.
In future studies, the suitability of this approach should
be applied to other paradigms, such as fog computing [77]
and evaluated for use side by side with edge devices. Another
interesting research topic is how to integrate decentralized ML
methods using federated learning or decentralized learning,
ensuring maximum security and transparency in the distributed
processes. This connection is a possibility considering that the
solution is based on an IoT platform that brings communication capabilities between all nodes. Another possibility is
to add support for building ML models for more heterogeneous devices, for example, edge devices, but with different
resources. This article has discussed using different programming languages, ML frameworks, and system architectures
(ARM and AMD); however, more options are available, such
as using GPUs, TPUs, or small microcontrollers like Arduino.
Authorized licensed use limited to: Monash University. Downloaded on September 19,2023 at 11:22:19 UTC from IEEE Xplore. Restrictions apply. 
16562 IEEE INTERNET OF THINGS JOURNAL, VOL. 10, NO. 18, 15 SEPTEMBER 2023
Using optimized models for each target architecture can
create various possibilities of developing more sustainable AI.
Creating and defining a monitoring pipeline that can evaluate
ML model performance (e.g., to establish model retraining
criteria) should also be explored in future research.
